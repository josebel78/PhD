{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/josebel78/03MIAR_Algoritmos-de-Optimizacion/blob/main/Algoritmos_Jose_Belenguer_AG3_reto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljZMs2rg93q6"
   },
   "source": [
    "# PhD-Paper-01-Cal\n",
    "## JosÃ© Belenguer Ballester\n",
    "### GitHub repository:\n",
    "#### https://github.com/josebel78/PhD.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-gbxt9BFfdN"
   },
   "source": [
    "## MODULE IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qP-Y3Gkfc4vn"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHRiJhTnFmuV"
   },
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_instance(file_name):\n",
    "        \n",
    "    global J, M, Rmax\n",
    "    \n",
    "    with open(file_name,\"r\") as file:\n",
    "        data = file.readlines()\n",
    "    \n",
    "    # PROBLEM SPECIFICATIONS: Line #1: number of machines (M), and of jobs (J)\n",
    "    specs = data[0].split()\n",
    "    J = int(specs[0])\n",
    "    M = int(specs[1])\n",
    "\n",
    "    machine_names = data[1].split()[0::2]\n",
    "\n",
    "    prec_lines = len(data) - (J+1)*2 - 2 # Number of lines for precedences in the instance file\n",
    "\n",
    "    # PROCESSING TIMES: lines # 2 ... 2+J-1\n",
    "    skiprows_times = 1\n",
    "    skipfooter_times = prec_lines + (J + 3)\n",
    "    processing_times_df = pd.read_csv(file_name,\n",
    "                                      sep=\"\\s+\",\n",
    "                                      header=None,\n",
    "                                      names=machine_names,\n",
    "                                      index_col=False,\n",
    "                                      usecols=list(range(1,2*M,2)),\n",
    "                                      dtype=np.int8,\n",
    "                                      engine='python',\n",
    "                                      skiprows=skiprows_times,\n",
    "                                      skipfooter=skipfooter_times\n",
    "                                     )\n",
    "\n",
    "    # RESOURCES: lines # 2 + J ... 2 * (J + 1)\n",
    "    skiprows_res = J + 2\n",
    "    skipfooter_res = prec_lines + 2\n",
    "    resources_df = pd.read_csv(file_name,\n",
    "                               sep=\"\\s+\",\n",
    "                               header=None,\n",
    "                               names=machine_names,\n",
    "                               index_col=False,\n",
    "                               usecols=list(range(1,2*M,2)),\n",
    "                               dtype=np.int8,\n",
    "                               engine='python',\n",
    "                               skiprows=skiprows_res,\n",
    "                               skipfooter=skipfooter_res\n",
    "                              )\n",
    "    \n",
    "    Rmax = int(data[(J+1)*2 + 1])\n",
    "    \n",
    "    # PRECEDENCES: lines # 2 + J ... 2 * (J + 1)\n",
    "    if prec_lines > 1: # If 1 then 'Precedence' would be an empty field\n",
    "        prec_array = np.full(shape=(J,), fill_value=None)\n",
    "        for p in range(len(data)-prec_lines+1,len(data)):\n",
    "            prec_line = data[p]\n",
    "            prec_line = prec_line.split(':')\n",
    "            prec_array[int(prec_line[0])] = int(prec_line[1])\n",
    "        \n",
    "    problem_df = pd.concat([processing_times_df, resources_df, pd.Series(prec_array)], axis=1, join='outer', copy=False)    \n",
    "    \n",
    "    print(f'\\nInstance specifications: J = {J} jobs, M = {M} machines, prec = {prec_lines-1} precedence relationship(s), and Rmax = {Rmax} resources.')\n",
    "\n",
    "    return problem_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA STRUCTURING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job:\n",
    "    def __init__(self, index, p_times, res, prec):\n",
    "        self.index = int(index) # Index (name) of the job as an int\n",
    "        self.p_times = p_times # Processing times on each machine as a NumPy array of float\n",
    "        self.res = res # Job resources on each machine as a NumPy array of float\n",
    "        self.prec = prec # Previous job (with a precedence relation) as an int\n",
    "        self.cost = int(0) # Job's assigned cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, index):\n",
    "        self.index = int(index) # Index (name) of the machine as an int\n",
    "        self.job_seq = np.empty(shape=(0)) #, dtype=np.int8) # Job sequence on the machine as a NumPy array\n",
    "        self.job_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.s_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.e_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.t_spans = {}\n",
    "        self.job_res = np.empty(shape=(0), dtype=np.int8) # Job resources on the machine at every instant as a NumPy array\n",
    "        self.C = int(0) # Machine makespan as an int\n",
    "        \n",
    "    def reset(self):\n",
    "        self.job_seq = np.empty(shape=(0)) #, dtype=np.int8) # Job sequence on the machine as a NumPy array\n",
    "        self.job_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.s_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.e_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.t_spans.clear()\n",
    "        self.job_res = np.empty(shape=(0), dtype=np.int8) # Job resources on the machine as a NumPy array\n",
    "        self.C = int(0) # Machine makespan as an int\n",
    "        \n",
    "    def program_job(self, job, pos=-1):\n",
    "        # Every parameter's length will be:\n",
    "            # s_times, e_times, job_times, job_seq: J\n",
    "            # job_res: C-1\n",
    "        if pos == -1:\n",
    "            job_s_time = int(0) if (self.job_seq.size == 0) else int(self.C)\n",
    "            job_e_time = job_s_time + job.p_times[self.index]\n",
    "            self.s_times = np.append(self.s_times, job_s_time)\n",
    "            self.e_times = np.append(self.e_times, job_e_time)\n",
    "            self.job_times = np.append(self.job_times, job.p_times[self.index])\n",
    "            self.job_res = np.append(self.job_res, job.res[self.index]*np.ones(shape=(job.p_times[self.index],)))\n",
    "            self.job_seq = np.append(self.job_seq, job)\n",
    "        else:\n",
    "            job_s_time = int(0) if (self.job_seq.size == 0) else int(self.s_times[pos])\n",
    "            job_e_time = job_s_time + job.p_times[self.index]\n",
    "            self.s_times = np.concatenate((self.s_times[:pos], \n",
    "                                           np.array([job_s_time]), \n",
    "                                           self.s_times[pos:] + job.p_times[self.index]))\n",
    "            self.e_times = np.concatenate((self.e_times[:pos], \n",
    "                                           np.array([job_e_time]), \n",
    "                                           self.e_times[pos:] + job.p_times[self.index]))\n",
    "            self.job_times = np.concatenate((self.job_times[:pos], \n",
    "                                             np.array([job.p_times[self.index]]), \n",
    "                                             self.job_times[pos:]))\n",
    "            self.job_res = np.concatenate((self.job_res[:job_s_time], \n",
    "                                             job.res[self.index]*np.ones(shape=(job.p_times[self.index],)), \n",
    "                                             self.job_res[job_s_time:]))\n",
    "            self.job_seq = np.concatenate((self.job_seq[:pos], \n",
    "                                           np.array([job]), \n",
    "                                           self.job_seq[pos:]))\n",
    "        self.t_spans.clear()\n",
    "        for j in range(len(self.job_seq)):\n",
    "            self.t_spans.update({self.job_seq[j].index: (self.s_times[j], self.e_times[j])})\n",
    "        self.C = self.e_times[-1] # Machine makespan as an int\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jobs(problem_df):\n",
    "    \n",
    "    # Creation of the job list\n",
    "    \n",
    "    job_list = []\n",
    "\n",
    "    for j in range(J):\n",
    "        index = j\n",
    "        p_times = np.array(list(problem_df.iloc[j,:M]))\n",
    "        res = np.array(list(problem_df.iloc[j,M:2*M]))\n",
    "        prec = int(problem_df.iloc[j,2*M]) if isinstance(problem_df.iloc[j,2*M], int) else problem_df.iloc[j,2*M]\n",
    "        job = Job(index, p_times, res, prec)\n",
    "        job_list.append(job)               \n",
    "\n",
    "    return job_list    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_machines(num_machines):\n",
    "    \n",
    "    # Creation of the machine list\n",
    "    \n",
    "    machine_list = [Machine(m) for m in range(num_machines)]\n",
    "\n",
    "    return machine_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_solution(solution):\n",
    "    \n",
    "    # Display of a solution\n",
    "    \n",
    "    for machine in solution:\n",
    "        print(f'\\nMachine M{machine.index}:')\n",
    "        print(f'Job sequence: \\t\\t {[job.index for job in machine.job_seq]}')\n",
    "        print(f'Start times: \\t\\t {machine.s_times}')\n",
    "        print(f'Processing times: \\t {machine.job_times}')\n",
    "        print(f'End times: \\t\\t {machine.e_times}')\n",
    "        print(f'Resources: \\t\\t {machine.job_res}')\n",
    "        print(f'Makespan: \\t\\t C = {machine.C}')\n",
    "\n",
    "    C_max = max([machine.C for machine in solution])\n",
    "    print(f'\\nThe maximum makespan is C_max: {C_max}')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION FEASIBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_feasibility(solution):\n",
    "        \n",
    "# def assess_feasibility(solution):\n",
    "    # Assesses whether a solution to the problem is feasible or unfeasible. Along with this condition, the function returns:\n",
    "        # If feasible, sol_cost = sol_Cmax.\n",
    "        # If unfeasible, sol_cost = sol_Cmax + (e_time_prec - s_time_dep) + abs(np.sum(acc_res - Rmax_res)).\n",
    "\n",
    "    # time_span_dict_m = {}\n",
    "    job_finder_dict = {}\n",
    "    dependency_dict = {}    \n",
    "\n",
    "    sol_Cmax = max([machine.C for machine in solution])\n",
    "    res_matrix = np.zeros(shape=(M,sol_Cmax))\n",
    "    sol_cost = sol_Cmax\n",
    "\n",
    "    prec_ok = True\n",
    "        \n",
    "    _ = [job_finder_dict.update({job.index: machine.index}) for machine in solution for job in machine.job_seq]\n",
    "\n",
    "    for machine in solution:\n",
    "        \n",
    "    # We iterate over every machine and create:\n",
    "        # For every job: an entry into the time_span_dict: key=job index, value=(start time, end time)\n",
    "        # For every job with a precedence relationship: an entry into the dependency_dict: key=dependent job's index, value=precedent job's index                \n",
    "        # For every job: a row in the res_matrix with the resource distribution over time (the whole time span of every machine)\n",
    "            \n",
    "        for pos in range(len(machine.job_seq)):\n",
    "            \n",
    "            if isinstance(machine.job_seq[pos].prec, int):\n",
    "                dep_job = machine.job_seq[pos].index\n",
    "                dep_machine_idx = machine.index\n",
    "                prec_job = machine.job_seq[pos].prec\n",
    "                prec_machine = job_finder_dict.get(prec_job)\n",
    "                dependency_dict.update({(dep_machine_idx,dep_job) : (prec_machine,prec_job)})\n",
    "\n",
    "        res_matrix[machine.index, :machine.C] = np.transpose(machine.job_res)\n",
    "\n",
    "    # Precedence relationships are analysed:\n",
    "        # Jobs with precedence relationships have to start once the precedent jobs have finalised.\n",
    "        # Otherwise, the prec_ok condition is set to False.\n",
    "        \n",
    "    for dep_job_info,prec_job_info in dependency_dict.items():\n",
    "        dep_machine = solution[dep_job_info[0]]\n",
    "        prec_machine = solution[prec_job_info[0]]        \n",
    "        s_time_dep = dep_machine.t_spans.get(dep_job_info[1])[0]\n",
    "        e_time_prec = prec_machine.t_spans.get(prec_job_info[1])[1]\n",
    "        if s_time_dep < e_time_prec:\n",
    "            prec_ok = False\n",
    "            sol_cost += (e_time_prec - s_time_dep)\n",
    "            break\n",
    "            \n",
    "    # Resource requirements are analysed:\n",
    "        # Cumulative use of resources in all machines at every instant of time is calculated.\n",
    "        # The res_ok condition is set to False as soon as Rmax is exceeded.\n",
    "        \n",
    "    acc_res = np.sum(res_matrix, axis=0)\n",
    "    Rmax_res = Rmax*np.ones_like(acc_res)\n",
    "    res_ok = True if np.all(acc_res <= Rmax) else False\n",
    "    sol_cost += 0 if res_ok else abs(np.sum(acc_res - Rmax_res))\n",
    "    \n",
    "    return int(sol_cost), prec_ok, res_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpwtaRQIFYvO"
   },
   "source": [
    "# CONSTRUCTIVE PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rcl(pre_sol, job_dict, rule, alpha):\n",
    "    \n",
    "    # Construction of a restricted candidate list (RCL) Pending jobs are sorted on every machine according to their cost value.\n",
    "\n",
    "    rcl_dict = {}\n",
    "    job_finder_dict = {}\n",
    "            \n",
    "    _ = [job_finder_dict.update({job.index: machine.index}) for machine in pre_sol for job in machine.job_seq]\n",
    "\n",
    "    \n",
    "    # Pending jobs are sorted on every machine according to their cost value.\n",
    "    \n",
    "    if rule == 'SPT':\n",
    "        for m in range(M):\n",
    "            for job in job_dict[m]:\n",
    "                job.cost = job.p_times[m] + pre_sol[m].C\n",
    "            rcl_dict[m] = sorted(job_dict[m], key=lambda x: x.p_times[m])\n",
    "         \n",
    "    elif rule == 'PREC':\n",
    "        for m in range(M):\n",
    "            p_time_avg = np.mean([job.p_times[m] for job in job_dict[m]])\n",
    "            for job in job_dict[m]:\n",
    "                job.cost = (job.p_times[m] + pre_sol[m].C) if not(isinstance(job.prec,int)) else (job.p_times[m] + pre_sol[m].C + p_time_avg)\n",
    "            rcl_dict[m] = sorted(job_dict[m], key=lambda x: x.cost)\n",
    "            \n",
    "    elif rule == 'RES':\n",
    "        for m in range(M):\n",
    "            p_time_avg = np.mean([job.p_times[m] for job in job_dict[m]])\n",
    "            for job in job_dict[m]:\n",
    "                job.cost = (job.p_times[m] + pre_sol[m].C + job.res[m]**2) if not(isinstance(job.prec,int)) else (job.p_times[m] + pre_sol[m].C + p_time_avg + job.res[m]**2)\n",
    "            rcl_dict[m] = sorted(job_dict[m], key=lambda x: x.cost)\n",
    "\n",
    "    \n",
    "    cost_list = [[job.cost for job in job_list_k] for k,job_list_k in job_dict.items()] # List of lists containing the pending jobs' costs on every machine\n",
    "    cost_array = np.asarray(cost_list)\n",
    "    \n",
    "    # Range of cost values to construct the RCL as a function of alpha.\n",
    "\n",
    "    cost_min = min(np.min(cost_array, axis=1))\n",
    "    cost_max = cost_min + alpha * (max(np.max(cost_array, axis=1)) - cost_min)\n",
    "    \n",
    "    # Pending jobs outside the limits of the RCL are removed.\n",
    "\n",
    "    for k,job in rcl_dict.items():\n",
    "        kj = len(rcl_dict[k]) - 1\n",
    "        while kj >= 0 and (job[kj].cost > cost_max):\n",
    "            rcl_dict[k].pop()\n",
    "            kj -= 1\n",
    "\n",
    "    return rcl_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tzrkaBS3gmmz"
   },
   "outputs": [],
   "source": [
    "def select_candidate(rcl_dict):\n",
    "    \n",
    "    candidate_machine_index = None\n",
    "    candidate_job_index = None\n",
    "    \n",
    "    candidate_machine_list = [machine_index for machine_index in rcl_dict.keys() if len(rcl_dict[machine_index]) > 0]\n",
    "    candidate_machine_index = random.choice(candidate_machine_list)\n",
    "    candidate_job_index = random.choice(rcl_dict[candidate_machine_index]).index\n",
    "\n",
    "    return candidate_machine_index, candidate_job_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tzrkaBS3gmmz"
   },
   "outputs": [],
   "source": [
    "def construct_initial_solution(job_list, rule, alpha):\n",
    "\n",
    "    # Creation of a dictionary with items defined by:\n",
    "        # keys: indices of the machines\n",
    "        # values: list of jobs sorted by the SPT rule (processing times in non-decreasing order on each machine)    \n",
    "    \n",
    "    job_dict = {}\n",
    "    for m in range(M):\n",
    "        job_dict.update({m: copy.deepcopy(job_list)})\n",
    "    \n",
    "    pending_jobs = J\n",
    "    \n",
    "    machine_env = create_machines(M)\n",
    "    _ = [machine.reset() for machine in machine_env]    \n",
    "    \n",
    "    while pending_jobs > 0:\n",
    "        \n",
    "        rcl_dict = construct_rcl(machine_env, job_dict, rule, alpha)        \n",
    "        candidate_machine_index, candidate_job_index = select_candidate(rcl_dict)\n",
    "        \n",
    "        # Assign the SPT job to the corresponding machine and remove it from the rest\n",
    "        for machine_index,jobs in job_dict.items():            \n",
    "            job_pos = [job.index for job in jobs].index(candidate_job_index) #[0][0]\n",
    "            candidate_job = jobs.pop(job_pos)\n",
    "            if machine_index == candidate_machine_index:\n",
    "                machine_env[machine_index].program_job(candidate_job)\n",
    "       \n",
    "        pending_jobs -= 1\n",
    "        \n",
    "    initial_Cmax, initial_prec_ok, initial_res_ok  = assess_feasibility(machine_env)\n",
    "    initial_feasibility = initial_prec_ok and initial_res_ok\n",
    "\n",
    "    return machine_env, initial_Cmax, initial_prec_ok, initial_res_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxyEjqm8YZIk"
   },
   "source": [
    "# LOCAL SEARCH PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_insertion(initial_solution, initial_Cmax, initial_feasibility, insert_time_limit):\n",
    "    \n",
    "    # Investigate neighbourhood of the initial solution (i.e. machine_environment generated in the constructive phase)    \n",
    "    \n",
    "    insert_time = 0\n",
    "    insert_start_time = time.monotonic()\n",
    "    \n",
    "    insert_time_exceeded = False\n",
    "    \n",
    "    sol_counter = 0\n",
    "    \n",
    "    if initial_feasibility:\n",
    "        best_f_solution = copy.deepcopy(initial_solution)\n",
    "        best_f_Cmax = initial_Cmax\n",
    "        partial_prec_ok = True\n",
    "        partial_res_ok = True\n",
    "        best_u_solution = []\n",
    "        best_u_Cmax = np.inf\n",
    "    else:\n",
    "        best_f_solution = []\n",
    "        best_f_Cmax = np.inf\n",
    "        best_u_solution = copy.deepcopy(initial_solution)\n",
    "        best_u_Cmax = initial_Cmax\n",
    "        partial_prec_ok = False\n",
    "        partial_res_ok = False\n",
    "    \n",
    "    for m0 in range(M):\n",
    "        \n",
    "        for j0 in range(len(initial_solution[m0].job_seq)):\n",
    "\n",
    "            extracted_job_0 = initial_solution[m0].job_seq[j0]\n",
    "                \n",
    "            for m1 in (set(range(M))-set([m0])):\n",
    "\n",
    "                for j1 in range(len(initial_solution[m1].job_seq)+1):\n",
    "\n",
    "                    partial_solution =  copy.deepcopy(initial_solution)\n",
    "\n",
    "                    partial_solution_0 = np.concatenate((initial_solution[m0].job_seq[:j0], initial_solution[m0].job_seq[j0+1:]))\n",
    "                    partial_solution[m0].reset()\n",
    "                    for job in partial_solution_0:\n",
    "                        partial_solution[m0].program_job(job)\n",
    "\n",
    "                    partial_solution_1 = np.concatenate((initial_solution[m1].job_seq[:j1], [extracted_job_0], initial_solution[m1].job_seq[j1:]))\n",
    "                    partial_solution[m1].reset()\n",
    "                    for job in partial_solution_1:\n",
    "                        partial_solution[m1].program_job(job)\n",
    "\n",
    "                    sol_counter += 1\n",
    "                    partial_Cmax, partial_prec_ok, partial_res_ok = assess_feasibility(partial_solution)\n",
    "                    partial_feasibility = partial_prec_ok and partial_res_ok\n",
    "    \n",
    "                    if partial_feasibility:\n",
    "                        if partial_Cmax < best_f_Cmax:\n",
    "                            best_f_solution = copy.deepcopy(partial_solution)\n",
    "                            best_f_Cmax = partial_Cmax\n",
    "                    else:\n",
    "                        if partial_Cmax < best_u_Cmax:\n",
    "                            best_u_solution = copy.deepcopy(partial_solution)\n",
    "                            best_u_Cmax = partial_Cmax\n",
    "                            \n",
    "                    del partial_solution\n",
    "                    \n",
    "                    insert_end_time = time.monotonic()\n",
    "                    insert_time = insert_end_time - insert_start_time\n",
    "                    insert_time_exceeded = insert_time >= insert_time_limit\n",
    "                    \n",
    "                    if insert_time_exceeded:\n",
    "                        break\n",
    "                        \n",
    "                if insert_time_exceeded:\n",
    "                    break\n",
    "                    \n",
    "            if insert_time_exceeded:\n",
    "                break\n",
    "                \n",
    "        if insert_time_exceeded:\n",
    "            break\n",
    "                \n",
    "    return partial_prec_ok, partial_res_ok, best_f_solution, best_f_Cmax, best_u_solution, best_u_Cmax, sol_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_swap(initial_solution, initial_Cmax, initial_feasibility, swap_time_limit):\n",
    "    \n",
    "    # Investigate neighbourhood of the initial solution (i.e. machine_environment generated in the constructive phase)\n",
    "    # The looping structure of the internal insertion is modified to avoid assessing twice the same swaps (e.g., 0<->1, 1<->0)\n",
    "    \n",
    "    # Timers\n",
    "    swap_time = 0\n",
    "    swap_start_time = time.monotonic()    \n",
    "    swap_time_exceeded = False\n",
    "    \n",
    "    # Solution counters\n",
    "    sol_counter = 0\n",
    "    \n",
    "    if initial_feasibility:\n",
    "        best_f_solution = copy.deepcopy(initial_solution)\n",
    "        best_f_Cmax = initial_Cmax\n",
    "        partial_prec_ok = True\n",
    "        partial_res_ok = True\n",
    "        best_u_solution = []\n",
    "        best_u_Cmax = np.inf\n",
    "    else:\n",
    "        best_f_solution = []\n",
    "        best_f_Cmax = np.inf\n",
    "        best_u_solution = copy.deepcopy(initial_solution)\n",
    "        best_u_Cmax = initial_Cmax\n",
    "        partial_prec_ok = False\n",
    "        partial_res_ok = False\n",
    "    \n",
    "    for m in range(M-1):\n",
    "        \n",
    "        m0 = m\n",
    "        \n",
    "        for j0 in range(len(initial_solution[m0].job_seq)):\n",
    "            \n",
    "            m1 = m0 + 1\n",
    "            \n",
    "            while m1 < M:\n",
    "                        \n",
    "                for j1 in range(len(initial_solution[m1].job_seq)):\n",
    "\n",
    "                    ################################################# EXTERNAL SWAP #################################################\n",
    "\n",
    "                    partial_solution =  copy.deepcopy(initial_solution)\n",
    "                    \n",
    "                    extracted_job_0 = initial_solution[m0].job_seq[j0]\n",
    "                    extracted_job_1 = initial_solution[m1].job_seq[j1]\n",
    "                    \n",
    "                    partial_solution_0 = np.concatenate((initial_solution[m0].job_seq[:j0], [extracted_job_1], initial_solution[m0].job_seq[j0+1:]))\n",
    "                    partial_solution[m0].reset()\n",
    "                    for job in partial_solution_0:\n",
    "                        partial_solution[m0].program_job(job)\n",
    "\n",
    "                    partial_solution_1 = np.concatenate((initial_solution[m1].job_seq[:j1], [extracted_job_0], initial_solution[m1].job_seq[j1+1:]))\n",
    "                    partial_solution[m1].reset()\n",
    "                    for job in partial_solution_1:\n",
    "                        partial_solution[m1].program_job(job)\n",
    "                    \n",
    "                    sol_counter += 1\n",
    "                    partial_Cmax, partial_prec_ok, partial_res_ok = assess_feasibility(partial_solution)\n",
    "                    partial_feasibility = partial_prec_ok and partial_res_ok\n",
    "\n",
    "                    if partial_feasibility:\n",
    "                        if partial_Cmax < best_f_Cmax:\n",
    "                            best_f_solution = copy.deepcopy(partial_solution)\n",
    "                            best_f_Cmax = partial_Cmax\n",
    "                    else:\n",
    "                        if partial_Cmax < best_u_Cmax:\n",
    "                            best_u_solution = copy.deepcopy(partial_solution)\n",
    "                            best_u_Cmax = partial_Cmax\n",
    "                            \n",
    "                    del partial_solution\n",
    "                    \n",
    "                    swap_end_time = time.monotonic()\n",
    "                    swap_time = swap_end_time - swap_start_time\n",
    "                    swap_time_exceeded = swap_time >= swap_time_limit\n",
    "                    \n",
    "                    if swap_time_exceeded:\n",
    "                        break\n",
    "                        \n",
    "                m1 += 1\n",
    "                \n",
    "                if swap_time_exceeded:\n",
    "                    break\n",
    "\n",
    "            if swap_time_exceeded:\n",
    "                break\n",
    "\n",
    "        if swap_time_exceeded:\n",
    "            # print('swap break level 1')\n",
    "            break\n",
    "                \n",
    "    return partial_prec_ok, partial_res_ok, best_f_solution, best_f_Cmax, best_u_solution, best_u_Cmax, sol_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search(initial_solution, initial_Cmax, initial_feasibility, local_time, local_time_limit, search_iter_limit, search_stagnation_limit):\n",
    "    \n",
    "    # Local search algorithm\n",
    "    \n",
    "    # Timers\n",
    "    local_time = 0    \n",
    "    local_start_time = time.monotonic()\n",
    "    \n",
    "    local_iter = 0\n",
    "    search_improvement = False\n",
    "    search_stagnation = 0\n",
    "    \n",
    "    best_feasible_dict = {}\n",
    "    best_unfeasible_dict = {}\n",
    "    \n",
    "    # Solution counters\n",
    "    local_sol_counter = 0\n",
    "    insert_counter = 0\n",
    "    swap_counter = 0\n",
    "\n",
    "    if initial_feasibility:\n",
    "        best_f_Cmax = initial_Cmax\n",
    "        best_feasible_dict.update({initial_Cmax: copy.deepcopy(initial_solution)})\n",
    "        best_u_Cmax = np.inf\n",
    "    else:\n",
    "        best_f_Cmax = np.inf\n",
    "        best_u_Cmax = initial_Cmax\n",
    "        best_unfeasible_dict.update({initial_Cmax: copy.deepcopy(initial_solution)})\n",
    "    \n",
    "    while (local_time < local_time_limit) and (search_stagnation < search_stagnation_limit):\n",
    "        \n",
    "        # External insertion\n",
    "        insert_time_limit = (local_time_limit - local_time) / 2\n",
    "        partial_prec_ok, partial_res_ok, partial_f_sol, partial_f_Cmax, partial_u_sol, partial_u_Cmax, insert_counter = external_insertion(initial_solution, \n",
    "                                                                                                                        initial_Cmax, \n",
    "                                                                                                                        initial_feasibility, \n",
    "                                                                                                                        insert_time_limit)\n",
    "        local_sol_counter += insert_counter\n",
    "\n",
    "        # Update solutions, if necessary. With the following structure, we prioritise feasible solutions (with respect to unfeasible ones)\n",
    "        if (len(partial_u_sol) > 0) and (partial_u_Cmax < best_u_Cmax):\n",
    "            best_unfeasible_dict.update({partial_u_Cmax: partial_u_sol})\n",
    "            best_u_Cmax = partial_u_Cmax\n",
    "            initial_solution = partial_u_sol\n",
    "            initial_Cmax = partial_u_Cmax\n",
    "            initial_feasibility = False\n",
    "            search_improvement = True\n",
    "        if (len(partial_f_sol) > 0) and (partial_f_Cmax < best_f_Cmax):\n",
    "            best_feasible_dict.update({partial_f_Cmax: partial_f_sol})\n",
    "            best_f_Cmax = partial_f_Cmax\n",
    "            initial_solution = partial_f_sol\n",
    "            initial_Cmax = partial_f_Cmax\n",
    "            initial_feasibility = True\n",
    "            search_improvement = True\n",
    "\n",
    "        # External swap\n",
    "        swap_time_limit = (local_time_limit - local_time) / 2\n",
    "        partial_prec_ok, partial_res_ok, partial_f_sol, partial_f_Cmax, partial_u_sol, partial_u_Cmax, swap_counter = external_swap(initial_solution, \n",
    "                                                                                                                                    initial_Cmax, \n",
    "                                                                                                                                    initial_feasibility, \n",
    "                                                                                                                                    swap_time_limit)\n",
    "        local_sol_counter += swap_counter\n",
    "\n",
    "        # Update solutions, if necessary. With the following structure, we prioritise feasible solutions (with respect to unfeasible ones)\n",
    "        if (len(partial_u_sol) > 0) and (partial_u_Cmax < best_u_Cmax):\n",
    "            best_unfeasible_dict.update({partial_u_Cmax: partial_u_sol})\n",
    "            best_u_Cmax = partial_u_Cmax\n",
    "            initial_solution = partial_u_sol\n",
    "            initial_Cmax = partial_u_Cmax\n",
    "            initial_feasibility = False\n",
    "            search_improvement = True\n",
    "        if (len(partial_f_sol) > 0) and (partial_f_Cmax < best_f_Cmax):\n",
    "            best_feasible_dict.update({partial_f_Cmax: partial_f_sol})\n",
    "            best_f_Cmax = partial_f_Cmax\n",
    "            initial_solution = partial_f_sol\n",
    "            initial_Cmax = partial_f_Cmax\n",
    "            initial_feasibility = True\n",
    "            search_improvement = True\n",
    "            \n",
    "        if not search_improvement:\n",
    "            search_stagnation += 1\n",
    "        else:\n",
    "            search_stagnation = 0\n",
    "            search_improvement = False\n",
    "        \n",
    "        local_iter += 1\n",
    "\n",
    "        local_end_time = time.monotonic()\n",
    "        local_time += (local_end_time - local_start_time)\n",
    "        \n",
    "    best_prec_ok = partial_prec_ok\n",
    "    best_res_ok = partial_res_ok\n",
    "        \n",
    "    return best_prec_ok, best_res_ok, best_feasible_dict, best_unfeasible_dict, local_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION REPAIRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_swap(unsorted_sol, m_index, dep_job_index, prec_job_index):\n",
    "    \n",
    "    # This function is called only when a dependent job is processed before its precedent job \n",
    "    sorted_sol = copy.deepcopy(unsorted_sol)\n",
    "    sorted_sol[m_index].reset()\n",
    "    \n",
    "    unsorted_job_list = [job for job in unsorted_sol[m_index].job_seq]\n",
    "    unsorted_job_list_indices = [job.index for job in unsorted_job_list]\n",
    "\n",
    "    dep_job_pos = unsorted_job_list_indices.index(dep_job_index)\n",
    "    prec_job_pos = unsorted_job_list_indices.index(prec_job_index)\n",
    "\n",
    "    # Firstly, jobs before the dependent one are re-programmed as they were\n",
    "    for pos, idx in enumerate(unsorted_job_list_indices):\n",
    "        \n",
    "        if idx == dep_job_index:\n",
    "            p = prec_job_pos\n",
    "        elif idx == prec_job_index:\n",
    "            p = dep_job_pos\n",
    "        else:\n",
    "            p = pos\n",
    "        \n",
    "        j = unsorted_job_list[p]\n",
    "            \n",
    "        sorted_sol[m_index].program_job(j)\n",
    "    \n",
    "    return sorted_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair(unfeasible_dict, repaired_prec_ok, repaired_res_ok, repair_time_limit):\n",
    "        \n",
    "    # To repair an unfeasible solution:\n",
    "        # Soluctions with precedence relationships are sought\n",
    "        # Starting times times of those jobs are compared against the finishing times of the precedent jobs \n",
    "    \n",
    "    repair_iter = 0\n",
    "    repair_time = 0\n",
    "    repair_prep_start_time = time.monotonic()\n",
    "    \n",
    "    job_finder_dict = {}\n",
    "    time_span_dict_m = {}\n",
    "    dependency_dict = {}\n",
    "    dependency_list = []\n",
    "    repaired_dict = {}\n",
    "    \n",
    "    best_u_Cmax = min(unfeasible_dict.keys())\n",
    "    unfeasible_sol = unfeasible_dict[best_u_Cmax]\n",
    "    repaired_sol = copy.deepcopy(unfeasible_sol)\n",
    "\n",
    "    sol_Cmax = max([machine.C for machine in unfeasible_sol])  \n",
    "    res_matrix = np.zeros(shape=(M,sol_Cmax))\n",
    "\n",
    "    # Precedence relationships are stored in a dictionary (dependency_dict) with the following info:\n",
    "        # key: a tuple comprising the dependent job's machine index and job index\n",
    "        # value: a tuple comprising a boolean stating whether both jobs are in the same machine, and the precedent job's index\n",
    "    # Parallelly, jobs' start and end times are stored as tuples in another dictionary (time_span_dict_m)\n",
    "    # Finally, the resources required by every job at every instant of time are stored in a matrix (res_matrix)\n",
    "        \n",
    "    _ = [job_finder_dict.update({job.index: machine.index}) for machine in unfeasible_sol for job in machine.job_seq]\n",
    "\n",
    "    for machine in unfeasible_sol:\n",
    "\n",
    "        for pos in range(len(machine.job_seq)):\n",
    "\n",
    "            if isinstance(machine.job_seq[pos].prec, int):\n",
    "                dep_job = machine.job_seq[pos].index\n",
    "                dep_machine_idx = machine.index\n",
    "\n",
    "                prec_job = machine.job_seq[pos].prec\n",
    "                prec_machine = job_finder_dict.get(prec_job)\n",
    "\n",
    "                dependency_dict[(dep_machine_idx,dep_job)] = (prec_machine,prec_job)\n",
    "\n",
    "                # A list of lists (dependency_list) is created to store interdependencies:\n",
    "                    # In the simplest case, every item (list) will contain two elements: dep_job, prec_job\n",
    "                    # In more complex situations, every item (list) will contain multiple elements: dep_job, prec_job_1, prec_job_2...\n",
    "\n",
    "                dependency_link = False\n",
    "\n",
    "                for s in range(len(dependency_list)):\n",
    "                    if dep_job in dependency_list[s]:\n",
    "                        dependency_list[s].append(prec_job)\n",
    "                        dependency_link = True\n",
    "                    elif prec_job in dependency_list[s]:\n",
    "                        # print(f'prec_job {prec_job} in dependency_list[{s}]')\n",
    "                        dependency_list[s].insert(0, dep_job)\n",
    "                        dependency_link = True\n",
    "\n",
    "                if not dependency_link:\n",
    "                    dependency_list.append([dep_job, prec_job])\n",
    "                    dependency_link = False                    \n",
    "\n",
    "        res_matrix[machine.index, :machine.C] = np.transpose(machine.job_res)\n",
    "\n",
    "    # A reversed version of the dependency_list is created:\n",
    "        # The first element [0] in every item (list) is the firt precedent job which will not be shifted\n",
    "        # The following elements [1:] in every item (list) will be ncrementally shifted\n",
    "\n",
    "    reversed_dependency_list = copy.deepcopy(dependency_list)\n",
    "\n",
    "    for item in reversed_dependency_list:\n",
    "        item.reverse()\n",
    "\n",
    "    # Precedence relationships are revised:\n",
    "        # If both jobs are in the same machine, and the dependent job is processed before the precedent job, they are immediately swapped\n",
    "        # In other cases, the precedence relationships will be fixed afterwards, whenever necessary\n",
    "\n",
    "    # The following processing would not be necessary if only simple precedence relationships existed. \n",
    "    # However, it becomes necessary when complex precedence relationships may arise.\n",
    "\n",
    "    dep_job_info_list = list(dependency_dict.keys()) # list of tuples (machine, dep_job)\n",
    "\n",
    "    idle_job = Job(index=int(J), p_times=np.ones(shape=(M,), dtype=np.int8), res=np.zeros(shape=(M,), dtype=np.int8), prec=None)\n",
    "    \n",
    "    repair_prep_end_time = time.monotonic()\n",
    "    repair_prep_time = (repair_prep_end_time - repair_prep_start_time)\n",
    "    repair_time += repair_prep_time\n",
    "    \n",
    "    repaired_sol_feasibility = repaired_prec_ok and repaired_res_ok\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    while (repair_time < repair_time_limit) and (not repaired_sol_feasibility):\n",
    "\n",
    "        repair_iter += 1\n",
    "        \n",
    "        print(f'Repair iteration starting at {round(repair_time, 4)} / {repair_time_limit} s        ', end='\\r')\n",
    "        \n",
    "        if not repaired_prec_ok:\n",
    "\n",
    "            ############################################### PRECEDENCE RELATIONSHIPS ###############################################\n",
    "\n",
    "            repair_prec_start_time = time.monotonic()\n",
    "            \n",
    "            for dep_seq in reversed_dependency_list:\n",
    "\n",
    "                for i in dep_seq[1:]:\n",
    "\n",
    "                    pos_list = [True if i == dep_job_tuple[1] else False for dep_job_tuple in dep_job_info_list]\n",
    "                    pos = pos_list.index(True)\n",
    "\n",
    "                    dep_job_info = dep_job_info_list[pos]\n",
    "                    prec_job_info = dependency_dict.get(dep_job_info)\n",
    "                    \n",
    "                    dep_machine = repaired_sol[dep_job_info[0]]\n",
    "                    prec_machine = repaired_sol[prec_job_info[0]]\n",
    "                    \n",
    "                    s_time_dep = dep_machine.t_spans.get(dep_job_info[1])[0]\n",
    "                    e_time_dep = dep_machine.t_spans.get(dep_job_info[1])[1]\n",
    "                    s_time_prec = prec_machine.t_spans.get(prec_job_info[1])[0]\n",
    "                    e_time_prec = prec_machine.t_spans.get(prec_job_info[1])[1]\n",
    "                    \n",
    "                    if s_time_dep < e_time_prec:\n",
    "\n",
    "                        if dep_machine == prec_machine:\n",
    "                            repaired_sol = internal_swap(repaired_sol, dep_job_info[0], dep_job_info[1], prec_job_info[1])\n",
    "                        else:\n",
    "                            job_indices = [j.index for j in dep_machine.job_seq]\n",
    "                            pos_dep_job = job_indices.index(dep_job_info[1])\n",
    "                            for d in range(e_time_prec-s_time_dep):\n",
    "                                dep_machine.program_job(idle_job, int(pos_dep_job))                            \n",
    "\n",
    "                        # Update the resource matrix, and ime spans in the corresponding dictionary:\n",
    "                        sol_Cmax = max([machine.C for machine in repaired_sol])\n",
    "                        res_matrix = np.zeros(shape=(M,sol_Cmax))\n",
    "\n",
    "                        for machine in repaired_sol:\n",
    "                            res_matrix[machine.index, :machine.C] = np.transpose(machine.job_res)\n",
    "\n",
    "                _, repaired_prec_ok, repaired_res_ok = assess_feasibility(repaired_sol)\n",
    "                repaired_sol_feasibility = repaired_prec_ok and repaired_res_ok\n",
    "\n",
    "                if repaired_prec_ok:\n",
    "                    break\n",
    "            \n",
    "            repair_prec_end_time = time.monotonic()\n",
    "            repair_prec_time = (repair_prec_end_time - repair_prec_start_time)\n",
    "            repair_time += repair_prec_time\n",
    "            \n",
    "        elif not repaired_res_ok:\n",
    "\n",
    "            ####################################################### RESOURCES #######################################################\n",
    "\n",
    "            # Resources exceeded: This happens when a new job starts\n",
    "            \n",
    "            repair_res_start_time = time.monotonic()\n",
    "\n",
    "            acc_res = np.sum(res_matrix, axis=0)\n",
    "            repaired_res_ok = True if np.all(acc_res <= Rmax) else False\n",
    "\n",
    "            p_x_res = np.where(acc_res > Rmax)[0][0] # Index where maximum resources are first exceeded. It will always be ind_x_res <= Cmax.\n",
    "\n",
    "            candidate_machine_idx = [machine.index for machine in repaired_sol if (p_x_res in machine.s_times)]\n",
    "            candidate_machine_C = [machine.C if (machine.index in candidate_machine_idx) else np.inf for machine in repaired_sol]\n",
    "            candidate_machine_C_idxd = list(enumerate(candidate_machine_C))\n",
    "            sorted_candidate_machine_C_idxd = sorted(candidate_machine_C_idxd, key=lambda x: x[1])\n",
    "\n",
    "            m_idx = 0\n",
    "\n",
    "            while (acc_res[p_x_res] > Rmax) and (sorted_candidate_machine_C_idxd[m_idx][1] < np.inf):\n",
    "                idle_m_index = sorted_candidate_machine_C_idxd[m_idx][0]\n",
    "                idle_j_pos = np.where(repaired_sol[idle_m_index].s_times == p_x_res)[0][0]\n",
    "            \n",
    "                repaired_sol[idle_m_index].program_job(idle_job, int(idle_j_pos))\n",
    "                sol_Cmax = max([machine.C for machine in repaired_sol])\n",
    "                \n",
    "                res_matrix = np.zeros(shape=(M,sol_Cmax))\n",
    "            \n",
    "                for machine in repaired_sol:\n",
    "    \n",
    "                    res_matrix[machine.index, :machine.C] = np.transpose(machine.job_res)               \n",
    "                \n",
    "                acc_res = np.sum(res_matrix, axis=0)\n",
    "            \n",
    "                m_idx += 1\n",
    "            \n",
    "            _, repaired_prec_ok, repaired_res_ok = assess_feasibility(repaired_sol)\n",
    "            repaired_sol_feasibility = repaired_prec_ok and repaired_res_ok\n",
    "        \n",
    "            repair_res_end_time = time.monotonic()\n",
    "            repair_res_time = (repair_res_end_time - repair_res_start_time)\n",
    "            repair_time += repair_res_time\n",
    "\n",
    "    repaired_dict.update({sol_Cmax : repaired_sol})    \n",
    "\n",
    "    return repaired_dict, repaired_prec_ok, repaired_res_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(solution):\n",
    "    \n",
    "    # Generate a solution\n",
    "\n",
    "    solution_dict = {}\n",
    "    len_job_seq = []\n",
    "    \n",
    "    for machine in solution:\n",
    "        job_list = [job.index for job in machine.job_seq]\n",
    "        start_times = machine.s_times\n",
    "        job_tuples = [(start_times[i], job_list[i]) for i in range(len(job_list)) if job_list[i] != J]\n",
    "        solution_dict.update({machine.index: job_tuples})\n",
    "        len_job_seq.append(len(job_tuples))\n",
    "        \n",
    "    \n",
    "    for k,v in solution_dict.items():\n",
    "        append_v = np.nan*np.ones(max(len_job_seq) - len(v))\n",
    "        new_v = v + append_v.tolist()\n",
    "        solution_dict.update({k:new_v})\n",
    "        \n",
    "    solution_df = pd.DataFrame.from_dict(solution_dict, orient='columns')\n",
    "    \n",
    "    return solution_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RIXYkDWcfmwd"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the number of the input dataset (0: debug, 1: cal, 2: test_small, 3: test_large):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input data path: C:\\Users\\Usuario\\Notebooks\\PhD\\Paper_01\\input_data\\cal\n",
      "\n",
      "instances: ['006x2_1-10_1_6.txt', '006x3_1-10_1_6.txt', '006x4_1-10_1_6.txt', '008x2_1-10_1_6.txt', '008x3_1-10_1_6.txt', '008x4_1-10_1_6.txt', '010x2_1-10_1_6.txt', '010x3_1-10_1_6.txt', '010x4_1-10_1_6.txt', '050x10_1-10_1_6.txt', '050x15_1-10_1_6.txt', '050x20_1-10_1_6.txt', '050x25_1-10_1_6.txt', '100x10_1-10_1_6.txt', '100x15_1-10_1_6.txt', '100x20_1-10_1_6.txt', '100x25_1-10_1_6.txt', '150x10_1-10_1_6.txt', '150x15_1-10_1_6.txt', '150x20_1-10_1_6.txt', '150x25_1-10_1_6.txt', '200x10_1-10_1_6.txt', '200x15_1-10_1_6.txt', '200x20_1-10_1_6.txt', '200x25_1-10_1_6.txt']\n",
      "\n",
      "\n",
      "############################################################## 006x2_1-10_1_6 #############################################################\n",
      "\n",
      "Instance specifications: J = 6 jobs, M = 2 machines, prec = 1 precedence relationship(s), and Rmax = 3 resources.\n",
      "\n",
      "Instance_time_limit: 12.0\n",
      "Iter_time_limit: 6.0\n",
      "GRASP iteration No. 18 starting at 11.54 / 12.0 s         \n",
      "Repair iteration starting at 0.0 / 6.0 s        \n",
      "Best solution: Cmax: 52, feasibility: True (precedence compliance: True, and resource compliance True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 5)</td>\n",
       "      <td>(0, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(34, 3)</td>\n",
       "      <td>(12, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(24, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(34, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1\n",
       "0   (0, 5)   (0, 4)\n",
       "1  (34, 3)  (12, 0)\n",
       "2      NaN  (24, 2)\n",
       "3      NaN  (34, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################################################## 006x3_1-10_1_6 #############################################################\n",
      "\n",
      "Instance specifications: J = 6 jobs, M = 3 machines, prec = 1 precedence relationship(s), and Rmax = 4 resources.\n",
      "\n",
      "Instance_time_limit: 18.0\n",
      "Iter_time_limit: 6.0\n",
      "GRASP iteration No. 14 starting at 14.105 / 18.0 s        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 186\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m################################################################## lOCAL SEARCH PHASE ##################################################################\u001b[39;00m\n\u001b[0;32m    184\u001b[0m search_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 186\u001b[0m neighbour_prec_ok, neighbour_res_ok, neighbour_feasible_dict, neighbour_unfeasible_dict, search_sol_counter \u001b[38;5;241m=\u001b[39m local_search(initial_solution, \n\u001b[0;32m    187\u001b[0m                                                                                                                           initial_Cmax, \n\u001b[0;32m    188\u001b[0m                                                                                                                           initial_feasibility, \n\u001b[0;32m    189\u001b[0m                                                                                                                           grasp_time, \n\u001b[0;32m    190\u001b[0m                                                                                                                           iter_T, \n\u001b[0;32m    191\u001b[0m                                                                                                                           search_iter_limit, \n\u001b[0;32m    192\u001b[0m                                                                                                                           search_stagnation_limit)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# If the local search returns any solutions, regardless being feasible and/or unfeasibles, they will improve Cmax and so we will add them to the dicts.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m feasible_sol_dict\u001b[38;5;241m.\u001b[39mupdate(neighbour_feasible_dict)\n",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m, in \u001b[0;36mlocal_search\u001b[1;34m(initial_solution, initial_Cmax, initial_feasibility, local_time, local_time_limit, search_iter_limit, search_stagnation_limit)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (local_time \u001b[38;5;241m<\u001b[39m local_time_limit) \u001b[38;5;129;01mand\u001b[39;00m (search_stagnation \u001b[38;5;241m<\u001b[39m search_stagnation_limit):\n\u001b[0;32m     31\u001b[0m     \n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# External insertion\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     insert_time_limit \u001b[38;5;241m=\u001b[39m (local_time_limit \u001b[38;5;241m-\u001b[39m local_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 34\u001b[0m     partial_prec_ok, partial_res_ok, partial_f_sol, partial_f_Cmax, partial_u_sol, partial_u_Cmax, insert_counter \u001b[38;5;241m=\u001b[39m external_insertion(initial_solution, \n\u001b[0;32m     35\u001b[0m                                                                                                                     initial_Cmax, \n\u001b[0;32m     36\u001b[0m                                                                                                                     initial_feasibility, \n\u001b[0;32m     37\u001b[0m                                                                                                                     insert_time_limit)\n\u001b[0;32m     38\u001b[0m     local_sol_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m insert_counter\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Update solutions, if necessary. With the following structure, we prioritise feasible solutions (with respect to unfeasible ones)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m, in \u001b[0;36mexternal_insertion\u001b[1;34m(initial_solution, initial_Cmax, initial_feasibility, insert_time_limit)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m1 \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(M))\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mset\u001b[39m([m0])):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(initial_solution[m1]\u001b[38;5;241m.\u001b[39mjob_seq)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 37\u001b[0m         partial_solution \u001b[38;5;241m=\u001b[39m  copy\u001b[38;5;241m.\u001b[39mdeepcopy(initial_solution)\n\u001b[0;32m     39\u001b[0m         partial_solution_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((initial_solution[m0]\u001b[38;5;241m.\u001b[39mjob_seq[:j0], initial_solution[m0]\u001b[38;5;241m.\u001b[39mjob_seq[j0\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]))\n\u001b[0;32m     40\u001b[0m         partial_solution[m0]\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\josebel78\\Lib\\copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(x, memo)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\josebel78\\Lib\\copy.py:196\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    194\u001b[0m append \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mappend\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m--> 196\u001b[0m     append(deepcopy(a, memo))\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\josebel78\\Lib\\copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m _reconstruct(x, memo, \u001b[38;5;241m*\u001b[39mrv)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\josebel78\\Lib\\copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m deepcopy(state, memo)\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\josebel78\\Lib\\copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(x, memo)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\josebel78\\Lib\\copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m deepcopy(value, memo)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\josebel78\\Lib\\copy.py:143\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    141\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(memo)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################## ALGORITHM CONFIGURATION ##################################################################\n",
    "\n",
    "parent_path = Path.cwd()\n",
    "\n",
    "input_data_path = parent_path / 'input_data'\n",
    "input_debug_path = input_data_path / 'debug'\n",
    "input_cal_path = input_data_path / 'cal'\n",
    "input_test_small_path = input_data_path / 'test_small'\n",
    "input_test_large_path = input_data_path / 'test_large'\n",
    "\n",
    "# input_debug_path.mkdir(exist_ok=True)\n",
    "# input_cal_path.mkdir(exist_ok=True)\n",
    "# input_test_small_path.mkdir(exist_ok=True)\n",
    "# input_test_large_path.mkdir(exist_ok=True)\n",
    "\n",
    "output_data_path = parent_path / 'output_data'\n",
    "output_debug_path = output_data_path / 'debug'\n",
    "output_cal_path = parent_path / 'output_data' / 'cal'\n",
    "output_test_small_path = parent_path / 'output_data' / 'test_small'\n",
    "output_test_large_path = parent_path / 'output_data' / 'test_large'\n",
    "\n",
    "output_data_path.mkdir(exist_ok=True)\n",
    "# output_test_small_path.mkdir(exist_ok=True)\n",
    "# output_test_large_path.mkdir(exist_ok=True)\n",
    "\n",
    "dataset_dict = {0: 'debug', 1: 'cal', 2: 'test_small', 3: 'test_large'}\n",
    "user_input = int(input('\\nEnter the number of the input dataset (0: debug, 1: cal, 2: test_small, 3: test_large): '))\n",
    "input_path = input_data_path / dataset_dict.get(user_input)\n",
    "output_path = output_data_path / dataset_dict.get(user_input)\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "instances = [instance.name for instance in sorted(input_path.glob('*.txt'))]\n",
    "\n",
    "print(f'\\nInput data path: {input_path}')\n",
    "print(f'\\nInstances: {instances}')\n",
    "\n",
    "os.chdir(input_path)\n",
    "\n",
    "if input_path == input_debug_path:\n",
    "\n",
    "    alpha_dict = {0: 0.00}\n",
    "    T_indices = list(range(1))\n",
    "    # T_indices = [2]\n",
    "    # alpha_dict = {0: 0.28}\n",
    "    # T_indices = list(range(3))\n",
    "\n",
    "elif input_path == input_cal_path:\n",
    "\n",
    "    alpha_dict = {0: 0.00, 1: 0.25, 2: 0.50, 3: 0.75, 4: 1.00}\n",
    "    T_indices = list(range(3))\n",
    "    # T_indices = [2]\n",
    "    # alpha_dict = {0: 0.28}\n",
    "    # T_indices = list(range(3))\n",
    "\n",
    "for alpha_index, problem_alpha in alpha_dict.items():\n",
    "    \n",
    "    for T_index in T_indices:\n",
    "\n",
    "        instance_sol = {}\n",
    "\n",
    "        for instance in instances:\n",
    "\n",
    "            instance_name = instance.rstrip('.txt')\n",
    "\n",
    "            print('\\n\\n' + str(' ' + instance_name + ' ').center(139, '#')) # 483\n",
    "\n",
    "            ################################################################## DATA LOADING ##################################################################\n",
    "\n",
    "            load_start_time = time.monotonic()\n",
    "\n",
    "            problem_df = read_instance(instance)\n",
    "\n",
    "            load_end_time = time.monotonic()\n",
    "\n",
    "            load_time = (load_end_time - load_start_time)\n",
    "\n",
    "            ################################################################## MULTI-START ##################################################################\n",
    "\n",
    "            random.seed(42)\n",
    "            g_rule = 'RES'\n",
    "\n",
    "            if input_path == input_debug_path:\n",
    "                T_dict = {0: M, 1: 2, 2: 1}\n",
    "            elif input_path == input_cal_path:\n",
    "                T_dict = {0: M, 1: 2, 2: 1}\n",
    "\n",
    "            search_iter_limit = J\n",
    "            search_stagnation_limit = J # J / 2\n",
    "\n",
    "            # Solution dictionaries\n",
    "            feasible_sol_dict = {}\n",
    "            unfeasible_sol_dict = {}\n",
    "            repaired_sol_dict = {}\n",
    "            best_grasp_f_dict = {}\n",
    "            best_grasp_u_dict = {}\n",
    "            best_grasp_sol_dict = {}\n",
    "\n",
    "            best_grasp_f_Cmax = np.inf\n",
    "            best_grasp_f_sol = []\n",
    "            best_grasp_f_dict.update({best_grasp_f_Cmax: best_grasp_f_sol})\n",
    "\n",
    "            best_grasp_u_Cmax = np.inf\n",
    "            best_grasp_u_sol = []\n",
    "            best_grasp_u_dict.update({best_grasp_u_Cmax: best_grasp_u_sol})\n",
    "\n",
    "            # Solution output\n",
    "            instance_Cmax = np.inf\n",
    "            instance_sol_df = pd.DataFrame()\n",
    "\n",
    "            # Iteration counters\n",
    "            grasp_iter = 0\n",
    "            grasp_improvement = False\n",
    "            grasp_stagnation = 0\n",
    "\n",
    "            # Solution counters\n",
    "            grasp_sol_counter_cum = 0\n",
    "            construct_sol_counter_cum = 0\n",
    "            search_sol_counter_cum = 0\n",
    "            repair_sol_counter_cum = 0\n",
    "            best_sol_counter_cum = 0\n",
    "\n",
    "            # Timers\n",
    "            grasp_time = 0\n",
    "            construct_time = 0\n",
    "            search_time = 0\n",
    "            repair_time = 0\n",
    "            best_time = 0\n",
    "\n",
    "            instance_T = float(J * M)\n",
    "            T_factor = T_dict.get(T_index)\n",
    "            iter_T = float(J * M) / T_factor\n",
    "            print(f'\\nInstance_time_limit: {instance_T}\\nIter_time_limit: {iter_T}')\n",
    "\n",
    "            best_start_time = time.monotonic()\n",
    "\n",
    "            while grasp_time < instance_T:\n",
    "\n",
    "                grasp_iter_start_time = time.monotonic()\n",
    "                grasp_iter += 1\n",
    "\n",
    "                print(f'GRASP iteration No. {grasp_iter} starting at {round(grasp_time, 4)} / {instance_T} s        ', end='\\r')\n",
    "\n",
    "                ################################################################## CONSTRUCTIVE PHASE ##################################################################\n",
    "\n",
    "                construct_start_time = time.monotonic()\n",
    "\n",
    "                job_list = create_jobs(problem_df)\n",
    "\n",
    "                # Every GRASP iteration starts with its own initial solution regardless whether it improves or not the previous ones.\n",
    "                # The purpose is to explore other areas of the solution space.\n",
    "\n",
    "                initial_solution, initial_Cmax, initial_prec_ok, initial_res_ok = construct_initial_solution(job_list, g_rule, problem_alpha)\n",
    "                initial_feasibility = initial_prec_ok and initial_res_ok\n",
    "                construct_sol_counter_cum += 1\n",
    "\n",
    "                if (initial_feasibility) and (initial_Cmax < best_grasp_f_Cmax):\n",
    "                    feasible_sol_dict.update({initial_Cmax: initial_solution})\n",
    "                    best_sol_counter_cum = construct_sol_counter_cum + search_sol_counter_cum\n",
    "                    best_end_time = time.monotonic()\n",
    "                    best_time = (best_end_time - best_start_time)\n",
    "                elif (not initial_feasibility) and (initial_Cmax < best_grasp_u_Cmax):\n",
    "                    unfeasible_sol_dict.update({initial_Cmax: initial_solution})        \n",
    "\n",
    "                construct_end_time = time.monotonic()\n",
    "                construct_time += (construct_end_time - construct_start_time)\n",
    "\n",
    "                grasp_time += construct_time\n",
    "\n",
    "                ################################################################## lOCAL SEARCH PHASE ##################################################################\n",
    "\n",
    "                search_start_time = time.monotonic()\n",
    "\n",
    "                neighbour_prec_ok, neighbour_res_ok, neighbour_feasible_dict, neighbour_unfeasible_dict, search_sol_counter = local_search(initial_solution, \n",
    "                                                                                                                                          initial_Cmax, \n",
    "                                                                                                                                          initial_feasibility, \n",
    "                                                                                                                                          grasp_time, \n",
    "                                                                                                                                          iter_T, \n",
    "                                                                                                                                          search_iter_limit, \n",
    "                                                                                                                                          search_stagnation_limit)\n",
    "\n",
    "                # If the local search returns any solutions, regardless being feasible and/or unfeasibles, they will improve Cmax and so we will add them to the dicts.\n",
    "                feasible_sol_dict.update(neighbour_feasible_dict)\n",
    "                unfeasible_sol_dict.update(neighbour_unfeasible_dict)\n",
    "                search_sol_counter_cum += search_sol_counter\n",
    "\n",
    "                if len(unfeasible_sol_dict) >= 1:\n",
    "\n",
    "                    best_u_Cmax = min(unfeasible_sol_dict.keys())\n",
    "\n",
    "                    if best_u_Cmax < best_grasp_u_Cmax:\n",
    "                        best_grasp_u_Cmax = best_u_Cmax\n",
    "                        best_grasp_u_sol = unfeasible_sol_dict.get(best_u_Cmax)\n",
    "                        best_grasp_u_dict.clear() # Comment this line if the nuimber of unfeasible solutions should be reduced to one\n",
    "                        best_grasp_u_dict.update({best_grasp_u_Cmax: best_grasp_u_sol})\n",
    "                        best_grasp_prec_ok = neighbour_prec_ok\n",
    "                        best_grasp_res_ok = neighbour_res_ok\n",
    "                        grasp_improvement = True\n",
    "\n",
    "                if len(feasible_sol_dict) >= 1:\n",
    "\n",
    "                    best_f_Cmax = min(feasible_sol_dict.keys())\n",
    "\n",
    "                    if best_f_Cmax < best_grasp_f_Cmax:\n",
    "                        best_grasp_f_Cmax = best_f_Cmax\n",
    "                        best_grasp_f_sol = feasible_sol_dict.get(best_f_Cmax)\n",
    "                        best_grasp_f_dict.clear() # Comment this line if the nuimber of feasible solutions should be reduced to one\n",
    "                        best_grasp_f_dict.update({best_grasp_f_Cmax: best_grasp_f_sol})\n",
    "                        best_sol_counter_cum = construct_sol_counter_cum + search_sol_counter_cum\n",
    "                        best_end_time = time.monotonic()\n",
    "                        best_time = (best_end_time - best_start_time)\n",
    "                        grasp_improvement = True\n",
    "\n",
    "                if not grasp_improvement:\n",
    "                    grasp_stagnation += 1\n",
    "                else:\n",
    "                    grasp_stagnation = 0\n",
    "                    grasp_improvement = False\n",
    "\n",
    "                search_end_time = time.monotonic()\n",
    "                search_time += (search_end_time - search_start_time)\n",
    "\n",
    "                grasp_iter_end_time = time.monotonic()\n",
    "                grasp_time += search_time\n",
    "\n",
    "            ##################################################################### REPAIR #####################################################################\n",
    "\n",
    "            if (len(best_grasp_f_dict) > 0) and (best_grasp_f_Cmax < np.inf):\n",
    "\n",
    "                instance_Cmax = best_grasp_f_Cmax\n",
    "                best_grasp_sol_dict.update({best_grasp_f_Cmax: best_grasp_f_dict.get(best_grasp_f_Cmax)})\n",
    "                best_sol_prec_ok = True\n",
    "                best_sol_res_ok = True\n",
    "                best_sol_feasibility = best_sol_prec_ok and best_sol_res_ok\n",
    "                repair_needed = False\n",
    "\n",
    "            elif len(best_grasp_u_dict) > 0:\n",
    "\n",
    "                repair_start_time = time.monotonic()\n",
    "                best_grasp_r_dict, best_sol_prec_ok, best_sol_res_ok = repair(best_grasp_u_dict, best_grasp_prec_ok, best_grasp_res_ok, iter_T)\n",
    "                best_sol_feasibility = best_sol_prec_ok and best_sol_res_ok\n",
    "                repair_end_time = time.monotonic()\n",
    "                repair_time = (repair_end_time - repair_start_time)\n",
    "                if best_sol_feasibility:\n",
    "                    best_sol_counter_cum = construct_sol_counter_cum + search_sol_counter_cum\n",
    "                    best_end_time = time.monotonic()\n",
    "                    best_time = (best_end_time - best_start_time)\n",
    "\n",
    "                best_grasp_r_Cmax = list(best_grasp_r_dict.keys())[0]\n",
    "                instance_Cmax = best_grasp_r_Cmax\n",
    "                repair_needed = True\n",
    "                best_grasp_sol_dict.update({best_grasp_r_Cmax: best_grasp_r_dict.get(best_grasp_r_Cmax)})\n",
    "            \n",
    "            print(f'\\nBest solution: Cmax: {instance_Cmax}, feasibility: {best_sol_feasibility} (precedence compliance: {best_sol_prec_ok}, and resource compliance {best_sol_res_ok})')\n",
    "            grasp_sol_counter_cum = construct_sol_counter_cum + search_sol_counter_cum + repair_sol_counter_cum\n",
    "\n",
    "            instance_sol.update({instance_name : [problem_alpha, \n",
    "                                                  instance_T, \n",
    "                                                  T_factor, \n",
    "                                                  instance_Cmax, \n",
    "                                                  best_sol_feasibility, \n",
    "                                                  repair_needed, \n",
    "                                                  round(best_time, 4), \n",
    "                                                  best_sol_counter_cum, \n",
    "                                                  grasp_sol_counter_cum, \n",
    "                                                  grasp_iter, \n",
    "                                                  round(construct_time, 4), \n",
    "                                                  round(search_time, 4), \n",
    "                                                  round(repair_time, 4)]})\n",
    "            \n",
    "            ##################################################################### GENERATE SOLUTION #####################################################################\n",
    "\n",
    "            instance_sol_df = generate_solution(best_grasp_sol_dict.get(instance_Cmax))\n",
    "            sol_name = instance_name + '_sol_T' + str(T_index) + '_a' + str(alpha_index) + '.csv'\n",
    "            sol_file_path = os.path.join(output_path, sol_name)\n",
    "            instance_sol_df.to_csv(sol_file_path, sep=';', header=True, index=True, mode='w', decimal=',')\n",
    "            display(instance_sol_df)\n",
    "            \n",
    "            ##################################################################### FINAL EVALUATION #####################################################################\n",
    "\n",
    "\n",
    "        print('\\n\\n' + ' SUMMARY '.center(139, '#')) #483\n",
    "\n",
    "        column_names = ['Alpha', 'T_limit', 'T_factor', 'Makespan', 'Feasibility', 'Repair', 'Time2best',\n",
    "                        'Sol2best', 'Total_sol', 'GRASP iter', 'Construct_time', 'Search_time', 'Repair_time']\n",
    "        summary_df = pd.DataFrame.from_dict(instance_sol, orient='index', dtype=None, columns=column_names)\n",
    "        display(summary_df)\n",
    "\n",
    "        os.chdir(output_path)\n",
    "        output_name = instance_type + '_T' + str(T_index) + '_a' + str(alpha_index) + '.csv'\n",
    "        output_file_path = output_path / output_name\n",
    "\n",
    "        summary_df.to_csv(output_file_path, sep=';', header=True, index=True, mode='w', decimal=',')\n",
    "\n",
    "        os.chdir(input_path)\n",
    "    \n",
    "os.chdir(parent_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
